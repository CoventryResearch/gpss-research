\documentclass[letterpaper]{article}
\usepackage{format/aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}  



% use Times
%\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
%\usepackage{natbib}

% For algorithms
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{algorithmicx}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
%\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{format/icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}

%\usepackage{times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{color}
\usepackage{preamble}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
    
\usepackage{amsmath, amsfonts, bm, lipsum, capt-of}
\usepackage{natbib, xcolor, wrapfig, booktabs, multirow, caption}
%\DeclareCaptionType{copyrightbox}
\usepackage{float}
\usepackage{datetime}

%\renewcommand{\baselinestretch}{0.99}

\def\ie{i.e.\ }
\def\eg{e.g.\ }
\let\oldemptyset\emptyset
\let\emptyset 0

\newcommand{\procedurename}{ABCD}
\newcommand{\genText}[1]{{\sf #1}}

%\author{
%James Robert Lloyd\\
%University of Cambridge\\
%Department of Engineering\\
%\texttt{jrl44@cam.ac.uk}
%\And
%David Duvenaud\\
%University of Cambridge \\
%Department of Engineering \\
%\texttt{dkd23@cam.ac.uk}
%\And
%Roger Grosse\\
%M.I.T.\\
%Brain and Cognitive Sciences \\
%\texttt{rgrosse@mit.edu}
%\And
%Joshua B. Tenenbaum\\
%M.I.T.\\
%Brain and Cognitive Sciences \\
%\texttt{jbt@mit.edu}
%\And
%Zoubin Ghahramani\\
%University of Cambridge \\
%Department of Engineering \\
%\texttt{zoubin@eng.cam.ac.uk}
%}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{0.6in}
\input{include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  % Note, NA's pass through!


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Automatic construction and natural-language description of nonparametric regression models}

\setcounter{secnumdepth}{2}

\begin{document} 

\title{Automatic Construction and Natural-Language Description \\ of Nonparametric Regression Models - Supplementary Material}
\author{}
\maketitle

%\appendix

\section{Kernels}

\subsection{Base kernels}

For scalar-valued inputs, the white noise ($\kWN$), constant ($\kC$), linear ($\kLin$), squared exponential ($\kSE$), and periodic kernels ($\kPer$) are defined as follows:
\begin{eqnarray}
\kWN(\inputVar, \inputVar') =& \sigma^2\delta_{\inputVar, \inputVar'} \\
\kC(\inputVar, \inputVar') =& \sigma^2 \\
\kLin(\inputVar, \inputVar') =& \sigma^2(\inputVar - \ell)(\inputVar' - \ell) \\
\kSE(\inputVar, \inputVar') =& \sigma^2\exp\left(-\frac{(\inputVar - \inputVar')^2}{2\ell^2}\right) \\
\kPer(\inputVar, \inputVar') =&  \sigma^2\frac{\exp\left(\frac{\cos\frac{2 \pi (\inputVar - \inputVar')}{p}}{\ell^2}\right) - I_0\left(\frac{1}{\ell^2}\right)}{\exp\left(\frac{1}{\ell^2}\right) - I_0\left(\frac{1}{\ell^2}\right)}
\end{eqnarray}
where $\delta_{\inputVar, \inputVar'}$ is the Kronecker delta function, $I_0$ is the modified Bessel function of the first kind of order zero and other symbols are parameters of the kernel functions.

\subsection{Changepoints and changewindows}

The changepoint, $\kCP(\cdot,\cdot)$ operator is defined as follows:
\begin{align}
\kCP(\kernel_1, \kernel_2)(x, x') = \qquad \qquad \sigma(x) & k_1(x,x')\sigma(x') \nonumber \\ + (1-\sigma(x)) & k_2(x,x')(1-\sigma(x'))
\end{align}
where $\sigma(x) = 0.5 \times (1 + \tanh(\frac{\ell - x}{s}))$.
This can also be written as
\begin{align}
\kCP(\kernel_1, \kernel_2) = \boldsymbol\sigma\kernel_1 + \boldsymbol{\bar\sigma}\kernel_2
\end{align}
where $\boldsymbol\sigma(x,x') = \sigma(x)\sigma(x')$ and $\boldsymbol{\bar\sigma}(x,x') = (1-\sigma(x))(1-\sigma(x'))$.

Changewindow, $\kCW(\cdot,\cdot)$, operators are defined similarly by replacing the sigmoid, $\sigma(x)$, with a product of two sigmoids.

\subsection{Properties of the periodic kernel}

A simple application of l'H\^opital's rule shows that
\begin{equation}
\kPer(x, x') \to \sigma^2\cos\left(\frac{2 \pi (x - x')}{p}\right) \quad \textrm{as} \quad\ell \to \infty.
\end{equation}
This limiting form is written as the cosine kernel ($\cos$).
%It was recently demonstrated \citep{WilAda13} that any stationary kernel can be arbitrarily well approximated by kernels with syntax\fTBD{RG: Syntax $\to$ expression, structure, family?}
%\begin{equation}
%\sum \kSE \times \cos
%\end{equation}
%by appealing to Bochner's theorem \citep{bochner1959lectures}.
%By using this new periodic kernel our language of kernels also attains this completeness property.

\section{Model construction / search}

\subsection{Overview}

The model construction phase of \procedurename{} starts with the kernel equal to the noise kernel, $\kWN$.
New kernel expressions are generated by applying search operators to the current kernel.
When new base kernels are proposed by the search operators, their parameters are randomly initialised with several restarts.
Parameters are then optimized by conjugate gradients to maximise the likelihood of the data conditioned on the kernel parameters.
The kernels are then scored by the Bayesian information criterion and the top scoring kernel is selected as the new kernel.
The search then proceeds by applying the search operators to the new kernel \ie this is a greedy search algorithm.

In all experiments, 10 random restarts were used for parameter initialisation and the search was run to a depth of 10.

\subsection{Search operators}

\procedurename{} is based on a search algorithm which used the following search operators
%
\begin{eqnarray}
\mathcal{S} &\to& \mathcal{S} + \mathcal{B} \\
\mathcal{S} &\to& \mathcal{S} \times \mathcal{B} \\
\mathcal{B} &\to& \mathcal{B'}
\end{eqnarray}
%
where $\mathcal{S}$ represents any kernel subexpression and $\mathcal{B}$ is any base kernel within a kernel expression \ie the search operators represent addition, multiplication and replacement.

To accommodate changepoint/window operators we introduce the following additional operators
%
\begin{eqnarray}
\mathcal{S} &\to& \kCP(\mathcal{S},\mathcal{S}) \\
\mathcal{S} &\to& \kCW(\mathcal{S},\mathcal{S}) \\
\mathcal{S} &\to& \kCW(\mathcal{S},\kC) \\
\mathcal{S} &\to& \kCW(\kC,\mathcal{S})
\end{eqnarray}
%
where $\kC$ is the constant kernel.
The last two operators result in a kernel only applying outside or within a certain region.

Based on experience with typical paths followed by the search algorithm we introduced the following operators
%
\begin{eqnarray}
\mathcal{S} &\to& \mathcal{S} \times (\mathcal{B} + \kC)\\
\mathcal{S} &\to& \mathcal{B}\\
\mathcal{S} + \mathcal{S'} &\to& \mathcal{S}\\
\mathcal{S} \times \mathcal{S'} &\to& \mathcal{S}
\end{eqnarray}
%
where $\mathcal{S'}$ represents any other kernel expression.
Their introduction is currently not rigorously justified.

\section{Predictive accuracy}

\paragraph{Interpolation}

To test the ability of the methods to interpolate, we randomly divided each data set into equal amounts of training data and testing data.
We trained each algorithm on the training half of the data, produced predictions for the remaining half and then computed the root mean squared error (RMSE).
The values of the RMSEs are then standardised by dividing by the smallest RMSE for each data set \ie the best performance on each data set will have a value of 1.

Figure~\ref{fig:box_interp} shows the standardised RMSEs for the different algorithms.
The box plots show that all quartiles of the distribution of standardised RMSEs are lower for both versions of \procedurename{}.
The median for \procedurename{}-accuracy is 1; it is the best performing algorithm on 7 datasets.
The largest outliers of \procedurename{} and spectral kernels are similar in value.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figures/box_interp}
\caption{
Box plot of standardised RMSE (best performance = 1) on 13 interpolation tasks.
}
\label{fig:box_interp}
\end{figure*}

Changepoints performs slightly worse than MKL despite being strictly more general than Changepoints.
The introduction of changepoints allows for more structured models, but it introduces parametric forms into the regression models (\ie the sigmoids expressing the changepoints).
This results in worse interpolations at the locations of the change points, suggesting that a more robust modeling language would require a more flexible class of changepoint shapes or improved inference (\eg fully Bayesian inference over the location and shape of the changepoint).

Eureqa is not suited to this task and performs poorly.
The models learned by Eureqa tend to capture only broad trends of the data since the fine details are not well explained by parametric forms.

\subsection{Tabels of standardised RMSEs}

See table~\ref{table:interp} for raw interpolation results and table~\ref{table:extrap} for raw extrapolation results. 
The rows follow the order of the datasets in the rest of the supplementary material.
The following abbreviations are used: \procedurename{}-accuracy (\procedurename{}-acc), \procedurename{}-interpretability ((\procedurename{}-int), Spectral kernels (SP), Trend-cyclical-irregular (TCI), Bayesian MKL (MKL), Eureqa (EL), Changepoints (CP), Squared exponential (SE) and Linear regression (Lin).

\begin{table*}[ht]
\center
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\procedurename{}-acc & \procedurename{}-int & SP & TCI & MKL & EL & CP & SE & Lin \\
\hline
1.04 & 1.00 & 2.09 & 1.32 & 3.20 & 5.30 & 3.25 & 4.87 & 5.01\\
1.00 & 1.27 & 1.09 & 1.50 & 1.50 & 3.22 & 1.75 & 2.75 & 3.26\\
1.00 & 1.00 & 1.09 & 1.00 & 2.69 & 26.20 & 2.69 & 7.93 & 10.74\\
1.09 & 1.04 & 1.00 & 1.00 & 1.00 & 1.59 & 1.37 & 1.33 & 1.55\\
1.00 & 1.06 & 1.08 & 1.06 & 1.01 & 1.49 & 1.01 & 1.07 & 1.58\\
1.50 & 1.00 & 2.19 & 1.37 & 2.09 & 7.88 & 2.23 & 6.19 & 7.36\\
1.55 & 1.50 & 1.02 & 1.00 & 1.00 & 2.40 & 1.52 & 1.22 & 6.28\\
1.00 & 1.30 & 1.26 & 1.24 & 1.49 & 2.43 & 1.49 & 2.30 & 3.20\\
1.00 & 1.09 & 1.08 & 1.06 & 1.30 & 2.84 & 1.29 & 2.81 & 3.79\\
1.08 & 1.00 & 1.15 & 1.19 & 1.23 & 42.56 & 1.38 & 1.45 & 2.70\\
1.13 & 1.00 & 1.42 & 1.05 & 2.44 & 3.29 & 2.96 & 2.97 & 3.40\\
1.00 & 1.15 & 1.76 & 1.20 & 1.79 & 1.93 & 1.79 & 1.81 & 1.87\\
1.00 & 1.10 & 1.03 & 1.03 & 1.03 & 2.24 & 1.02 & 1.77 & 9.97\\
\hline
\end{tabular}
\caption{Interpolation standardised RMSEs}
\label{table:interp}
\end{table*}

\begin{table*}[ht]
\center
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\procedurename{}-acc & \procedurename{}-int & SP & TCI & MKL & EL & CP & SE & Lin \\
\hline
1.14 & 2.10 & 1.00 & 1.44 & 4.73 & 3.24 & 4.80 & 32.21 & 4.94\\
1.00 & 1.26 & 1.21 & 1.03 & 1.00 & 2.64 & 1.03 & 1.61 & 1.07\\
1.40 & 1.00 & 1.32 & 1.29 & 1.74 & 2.54 & 1.74 & 1.85 & 3.19\\
1.07 & 1.18 & 3.00 & 3.00 & 3.00 & 1.31 & 1.00 & 3.03 & 1.02\\
1.00 & 1.00 & 1.03 & 1.00 & 1.35 & 1.28 & 1.35 & 2.72 & 1.51\\
1.00 & 2.03 & 3.38 & 2.14 & 4.09 & 6.26 & 4.17 & 4.13 & 4.93\\
2.98 & 1.00 & 11.04 & 1.80 & 1.80 & 493.30 & 3.54 & 22.63 & 28.76\\
3.10 & 1.88 & 1.00 & 2.31 & 3.13 & 1.41 & 3.13 & 8.46 & 4.31\\
1.00 & 2.05 & 1.61 & 1.52 & 2.90 & 2.73 & 3.14 & 2.85 & 2.64\\
1.00 & 1.45 & 1.43 & 1.80 & 1.61 & 1.97 & 2.25 & 1.08 & 3.52\\
2.16 & 2.03 & 3.57 & 2.23 & 1.71 & 2.23 & 1.66 & 1.89 & 1.00\\
1.06 & 1.00 & 1.54 & 1.56 & 1.85 & 1.93 & 1.84 & 1.66 & 1.96\\
3.03 & 4.00 & 3.63 & 3.12 & 3.16 & 1.00 & 5.83 & 5.35 & 4.25\\
\hline
\end{tabular}
\caption{Extrapolation standardised RMSEs}
\label{table:extrap}
\end{table*}

\section{Examples of automatically generated reports}

Concatenated with this supplementary material are 13 reports automatically generated by \procedurename{}.

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\end{document} 
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

\section{Improvements to Language}

\subsubsection{Changepoints and changewindows}

Old GPSS could not capture changepoints evidenced by these solar irradiance data residuals in figure~\ref{fig:not_cp}.
New GPSS can do changepoints (see later section).

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/old-gpss/02-solar-s_resid}
\caption{Old GPSS - these residuals need a changepoint.}
\label{fig:not_cp}
\end{figure}

\subsubsection{Removing constants}

The first two components in the old GPSS decomposition of airline were
\begin{itemize}
  \item $\kSE \times \kLin_1'$
  \item $\kSE \times \kLin_2' \times \kPer'$
\end{itemize}
where primes represent old defintions of the kernels.
The linear kernels have different parameters (indicated by subscripts) but the $\kSE$ is the same in both products.

Using the new versions of the kernels (removing constants) these expressions become
\begin{itemize}
  \item $\kSE \times \kLin_1 + \kSE$
  \item $\kSE \times \kLin_2 \times \kPer + \kSE \times \kLin_2 +  \kSE \times \kPer + \kSE$
\end{itemize}
which is undesirable in two ways.
Firstly, both of these expressions contain the same $\kSE$ as an additive component resulting in anti-correlation in the posterior; contrast figures \ref{fig:anti_corr_1} and \ref{fig:anti_corr_2} with figures \ref{fig:monotonic} and \ref{fig:lin_periodic}.

Secondly, the second component is a sum of four terms and is therefore rather a mouthful to describe.
It is the sum of an approximately periodic function with approximately linearly increasing amplitude, a smooth function with linearly increasing marginal standard deviation, an approximately periodic function and a smooth function!

The removal of constants results in base kernels which are atomic from the point of view of description \fTBD{In the way they are currently translated at least} and they make anti-correlation less likely\fTBD{however, they are not orthogonal in any sense so anti-correlation can still happen - I'm not sure how to avoid this or whether it is inevitable}.

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/old-gpss/01-airline-months_1}
\caption{Old GPSS - uncertainty is in fact anti-correlation.}
\label{fig:anti_corr_1}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/old-gpss/01-airline-months_2}
\caption{Old GPSS - uncertainty is in fact anti-correlation.}
\label{fig:anti_corr_2}
\end{figure}

\paragraph{Removal of $\kRQ$}
The $\kRQ$ kernel is equivalent to a mixture of $\kSE$ covariance functions with different length-scales.
In preliminary testing we observed that this allowed the kernel to simultaneously model aspects of the data that might be considered signal and noise.
We therefore removed this kernel to improve the interpretability of the models produced.

\subsubsection{Heteroscedasticity}

In the original GPSS the heteroscedasticity of the airline dataset was partially captured by the $\kLin \times \kSE \times \kRQ$ term which was modeling short term variation.
This term was also modeling the medium term variations in the data (an example of $\kRQ$ capturing multiple lengthscales) forcing a compromise of parameter values\fTBD{This may also be a result of not distributing products during the search - not entirely sure} resulting in the variance of the residuals decreasing (see figure~\ref{fig:not_hetero} from old GPSS) rather than increasing with the data (see new figure~\ref{fig:hetero}).\fTBD{The noise variance is too high due to a mathematical error in the original version}

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/old-gpss/01-airline-months_resid}
\caption{Old GPSS - heteroscedasticity not captured and variance larger than residuals.}
\label{fig:not_hetero}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/01-airline/01-airline_3_anti_cum}
\caption{New GPSS - heteroscedasticity captured and mostly appropriate.}
\label{fig:hetero}
\end{figure}

Introducing $\kWN$ as a base kernel allows heteroscedasticity to be explicitly modelled\fTBD{we now have a delta likelihood function rather than always assuming homoscedastic Gaussian noise} rather than having to approximate it with components that can represent short lengthscales.




\subsubsection{Removal of $\kRQ$}

GPML describes the $\kRQ$ as ``a scale mixture of \kSE covariance functions with different characteristic length-scales''.
To exactly specify this scale mixture would require an equation or plot.
One could summarise this information with descriptions like `90\% of the length-scales are between $x$ and $y$' but I do not think there is any common language that would express what is happening exactly.

Also the $\kRQ$ kernel can model smooth trends and very short term variation in one component.
What was previously one component (figure \ref{fig:RQ}) is now two (figures \ref{fig:medium} and \ref{fig:short}) in new GPSS.
This feels more interpretable to me, but this is subjective and maybe subtle.

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/old-gpss/03-mauna2003-s_3}
\caption{Old GPSS - many lengthscales in one component.}
\label{fig:RQ}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/03-mauna/03-mauna_3}
\caption{New GPSS - medium term variation.}
\label{fig:medium}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/03-mauna/03-mauna_5}
\caption{New GPSS - very short term variation.}
\label{fig:short}
\end{figure}

More convincing hopefully is the plot in figure~\ref{fig:RQ_bad} which shows a fit to data representing sales of mink fur.
The kernel is $\kRQ \times \kPer$\fTBD{redundant multiplicative constant kernels were not removed in the version of the algorithm that produced this figure}.
The extrapolation shows that it has captured the near exact periodicity (\ie a long lengthscale phenomenon) but the interpolation shows it has also captured short term variation (it has joined the dots!).
Combining these scales of variation does not seem interpretable\fTBD{I tried for quite some time to avoid this fit to the data before convincing myself that $\kRQ$ was the problem rather than anything else}.

\begin{figure}[h]
\centering
\includegraphics[width=0.98\columnwidth]{figures/old-gpss/fur-sales-mink-h-b-co-18481911_1}
\caption{Old GPSS - the trend and noise in one component.}
\label{fig:RQ_bad}
\end{figure}

\subsection{An improved periodic kernel}

We introduce a new periodic kernel defined by
\begin{equation}
\kernel_\kPer(x, x') = \kernel_\kPer(\tau) =  \sigma^2\frac{\exp\left(\frac{\cos\frac{2 \pi \tau}{p}}{\ell^2}\right) - I_0\left(\frac{1}{\ell^2}\right)}{\exp\left(\frac{1}{\ell^2}\right) - I_0\left(\frac{1}{\ell^2}\right)}
\label{eq:periodic}
\end{equation}
where $\tau := x - x'$ and $I_0$ is the modified Bessel function of the first kind of order 0.
It is linearly related to the original periodic kernel \TBD{cite something?} but has the following elegant properties.

\paragraph{Cosine limit}



\paragraph{No power at zero frequency}

The kernel defined by equation~\eqref{eq:periodic} has a Fourier transform of zero when evaluated at zero.
Therefore, functions drawn from a \gp{} with this kernel will have zero mean.
In contrast, the previous periodic kernel defined a prior over a zero mean periodic function plus an independent constant function.
For the purposes of description we find it more interpretable to separate zero mean periodicity and constant functions.
\NA{
cite Sheffield's orthogonal RKHS work?
}

\section{Derivation of Component Marginal Variance}

In this section, we derive the posterior marginal variance and covariance of the additive components of a \gp{}.  These formulas let us plot the marginal variance of each component separately.  These formulas can also be used to examine the posterior covariance between pairs of components.

Let us assume that our function $\vf$ is a sum of two functions, $\vf_1$ and $\vf_2$, where $\vf = \vf_1 + \vf_2$.  If $\vf_1$ and $\vf_2$ are a priori independent, and $\vf_1 \sim \gp( \vmu_1, k_1)$ and $\vf_2 \sim \gp( \vmu_2, k_2)$, then
\begin{align}
\left[ \begin{array}{c} \vf_1 \\ \vf_1^\star \\ \vf_2 \\ \vf_2^\star \\ \vf \\ \vf^\star \end{array} \right]
\sim
\Nt{
\left[ \begin{array}{c} \vmu_1 \\ \vmu_1^\star \\ \vmu_2 \\ \vmu_2^\star \\ \vmu_1 + \vmu_2 \\ \vmu_1^\star + \vmu_2^\star \end{array} \right]
}
{K
}
\end{align}

where
\begin{align}
 K = \left[ \begin{array}{cccccc} 
\vk_1 & \vk_1^\star & 0 & 0 & \vk_1 & \vk_1^\star \\ 
\vk_1^\star & \vk_1^{\star\star} & 0 & 0 & \vk_1^\star & \vk_1^{\star\star} \\
0 & 0 & \vk_2 & \vk_2^\star & \vk_2 & \vk_2^\star \\ 
0 & 0 & \vk_2^\star & \vk_2^{\star\star} & \vk_2^\star & \vk_2^{\star\star} \\
\vk_1 & \vk_1^\star & \vk_2 & \vk_2^\star & \vk_1 + \vk_2 & \vk_1^\star + \vk_2^\star \\ 
\vk_1^\star & \vk_1^{\star\star}  & \vk_2^\star & \vk_2^{\star\star}  & \vk_1^\star + \vk_2^\star & \vk_1^{\star\star} + \vk_2^{\star\star}\\
\end{array} \right]
\end{align}

and 

\begin{align}
\vk_1 & = k_1( \vX, \vX ) \\
\vk_1^\star & = k_1( \vX^\star, \vX ) \\
\vk_1^{\star\star} & = k_1( \vX^\star, \vX^\star )
\end{align} 

By the formula for Gaussian conditionals:
\begin{align}
\vx_A | \vx B \sim \mathcal{N} \big( & \vmu_A + \vSigma_{AB} \vSigma_{BB}\inv \left( \vx_B - \vmu_B \right), \\
& \vSigma_{AA} - \vSigma_{AB} \vSigma_{BB}\inv \vSigma_{BA} \big),
\end{align}
we get that the conditional variance of a Gaussian conditioned on its sum with another Gaussian is given by
\begin{align}
\vf_1^\star | \vf \sim \mathcal{N} \big( &\vmu_1^\star + \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \left( \vf - \vmu_1 - \vmu_2 \right), \\
 & \vk_1^{\star\star} - \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_1^\star \big).
\end{align}

The covariance between the two components, conditioned on their sum is given by:
\begin{align}
\cov(\vf_1^\star, \vf_2^\star) | \vf = - \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_2^\star
\end{align}

These formulae express the posterior model uncertainty about different components of the signal, integrating over the possible configurations of the other components.

\section{Derivation of Component Marginal Variance}

Let us assume that our function $\vf$ is a sum of two functions, $\vf_1$ and $\vf_2$, where $\vf = \vf_1 + \vf_2$.  If $\vf_1$ and $\vf_2$ are a priori independent, and $\vf_1 \sim \gp( \vmu_1, k_1)$ and $\vf_2 \sim \gp( \vmu_2, k_2)$, then
\begin{align}
\left[ \begin{array}{c} \vf_1 \\ \vf_1^\star \\ \vf_2 \\ \vf_2^\star \\ \vf \\ \vf^\star \end{array} \right]
\sim
\Nt{
\left[ \begin{array}{c} \vmu_1 \\ \vmu_1^\star \\ \vmu_2 \\ \vmu_2^\star \\ \vmu_1 + \vmu_2 \\ \vmu_1^\star + \vmu_2^\star \end{array} \right]
}
{\left[ \begin{array}{cccccc} 
\vk_1 & \vk_1^\star & 0 & 0 & \vk_1 & \vk_1^\star \\ 
\vk_1^\star & \vk_1^{\star\star} & 0 & 0 & \vk_1^\star & \vk_1^{\star\star} \\
0 & 0 & \vk_2 & \vk_2^\star & \vk_2 & \vk_2^\star \\ 
0 & 0 & \vk_2^\star & \vk_2^{\star\star} & \vk_2^\star & \vk_2^{\star\star} \\
\vk_1 & \vk_1^\star & \vk_2 & \vk_2^\star & \vk_1 + \vk_2 & \vk_1^\star + \vk_2^\star \\ 
\vk_1^\star & \vk_1^{\star\star}  & \vk_2^\star & \vk_2^{\star\star}  & \vk_1^\star + \vk_2^\star & \vk_1^{\star\star} + \vk_2^{\star\star}\\
\end{array} \right]
}
\end{align}
where $\vk_1 = k_1( \vX, \vX )$ and $\vk_1^\star = k_1( \vX^\star, \vX )$. 

By the formula for Gaussian conditionals:
\begin{align}
\vx_A | \vx B \sim \Nt{\vmu_A + \vSigma_{AB} \vSigma_{BB}\inv \left( \vx_B - \vmu_B \right) }
{\vSigma_{AA} - \vSigma_{AB} \vSigma_{BB}\inv \vSigma_{BA} },
\end{align}
we get that the conditional variance of a Gaussian conditioned on its sum with another Gaussian is given by
\begin{align}
\vf_1^\star | \vf \sim \Nt{\vmu_1^\star + \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \left( \vf - \vmu_1 - \vmu_2 \right) }
{\vk_1^{\star\star} - \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_1^\star }.
\end{align}

The covariance between the two components, conditioned on their sum is given by:
\begin{align}
\cov(\vf_1^\star, \vf_2^\star) | \vf = - \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_2^\star
\end{align}

These formulae express the posterior model uncertainty about different components of the signal, integrating over the possible configurations of the other components.