\input{header_beamer}
\usepackage{etex}
%\include{macros}
%\documentclass[usenames,dvipsnames]{beamer}
%\usepackage{beamerthemesplit}
%\usepackage{graphics}
%\usepackage{amsmath}
%\usepackage{rotating}
%\usepackage{array}
%\usepackage{nth}
\usepackage{xcolor}
\usepackage{textcomp}
\input{matlab_setup}

\usepackage{tabularx}
\usepackage{picins}
\usepackage{tikz}

\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]

\definecolor{camlightblue}{rgb}{0.601 , 0.8, 1}
\definecolor{camdarkblue}{rgb}{0, 0.203, 0.402}
\definecolor{camred}{rgb}{1, 0.203, 0}
\definecolor{camyellow}{rgb}{1, 0.8, 0}
\definecolor{lightblue}{rgb}{0, 0, 0.80}
\definecolor{white}{rgb}{1, 1, 1}
\definecolor{whiteblue}{rgb}{0.80, 0.80, 1}

\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\tabbox}[1]{#1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Some look and feel definitions
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\columnsep}{0.03\textwidth}
\setlength{\columnseprule}{0.0018\textwidth}
\setlength{\parindent}{0.0cm}

%\include{macros}
\usepackage{preamble}
\hypersetup{colorlinks=true,citecolor=blue}
%\pdfmapfile{+sansmathaccent.map}

\title{Automatic construction and description of nonparametric models}

\author{
\includegraphics[height=0.2\textwidth]{figures/JamesLloyd3}
\qquad
\includegraphics[height=0.2\textwidth]{figures/david}
\qquad
\includegraphics[height=0.2\textwidth]{figures/roger-photo}
\\
James Robert Lloyd, David Duvenaud, Roger Grosse,\\ Josh Tenenbaum, Zoubin Ghahramani
}
\institute{
%\includegraphics[width=0.4\textwidth]{figures/spiral_main}
}
%\date{}


\begin{document}

\frame[plain] {
\titlepage
}

\setbeamercolor{toc}{fg=black}

%\frame[plain] {
%\frametitle{Outline}
%\tableofcontents

%\begin{itemize} 
%	\item Motivation
%	\item Automated structure discovery in regression
%	\begin{itemize} 
%		\item Gaussian process regression
%		\item Structures expressible through kernel composition
%		\item A massive missing piece
%		\item grammar \& search over models
%		\item Examples of structures discovered
%	\end{itemize}
%	\item Automated structure discovery in matrix models
%	\begin{itemize} 
%		\item expressing models as matrix decompositions
%		\item grammar \& special cases
%		\item examples of structures discovered on images
%	\end{itemize}
%\end{itemize}   
%
%}


%\frame[plain]{
%\frametitle{Credit where credit is due}
%
%Talk based on two papers:
%	\begin{itemize}
%		\item Structure Discovery in Nonparametric Regression through Compositional Kernel Search [ICML 2013]
%		\\
%		{David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani}
%		\item Exploiting compositionality to explore a large space of model structures [UAI 2012]
%		\\
%		Roger B. Grosse, Ruslan Salakhutdinov, \\William T. Freeman, Joshua B. Tenenbaum
%	\end{itemize}
%}


\frame[plain]{
\frametitle{Motivation}
\begin{itemize} 
	\item Models today built by hand, or chosen from a fixed set.
	\begin{itemize} 
		\item Example: Scikit-learn
		\item \includegraphics[width=8cm, trim=1cm 15cm 35cm 0cm, clip]{figures/plot_classifier_comparison_1}
		\item \includegraphics[width=8cm, trim=35cm 15cm 1cm 0cm, clip]{figures/plot_classifier_comparison_1}
%		\begin{itemize} 
		\item Just being nonparametric sometimes isn't good enough 
%			\item to learn efficiently, need to have a rich prior that can express most of the structure in your data.
%		\end{itemize}
		\item Building by hand requires expertise, understanding of the dataset.
	\end{itemize}
\end{itemize}
}




\frame[plain]{
\frametitle{Motivation}
	\begin{itemize} 
	\item Models today built by hand, or chosen from a fixed set.
	\begin{itemize} 

		\item Building by hand requires expertise, understanding of the dataset.
		
		\item Follows cycle of: propose model, do inference, check model fit
		\begin{itemize} 
			\item Propose new model
			\item Do inference
			\item Check model fit
		\end{itemize}
		\item for high-dimensional data, this can silently fail
	\end{itemize}
	\item Andrew Gelman asks:  How would an AI do statistics?
	\item It would need a language for describing arbitrarily complicated models, a way to search over those models, nad a way of checking model fit.
\end{itemize}
}




\frame[plain]{
\frametitle{Motivation}
	\begin{itemize} 
	\item Andrew Gelman asks:  How would an AI do statistics?
	\item It would need a language for describing arbitrarily complicated models, a way to search over those models, nad a way of checking model fit.
	\item We built such a language over regression models, a procedure to search over them, and a method to describe in english language the properties of the resulting models.
\end{itemize}
}


\definecolor{verylightblue}{rgb}{0.97,0.97,1}
\setlength{\fboxsep}{0pt}

\newcommand{\ltrim}{ 2 }
\newcommand{\rictrim}{ 2 }
%\newcommand{\airlinefig}[1]{\includegraphics[trim=20 0 12 20, clip, width=0.207\textwidth]{figures/#1}}
%\newcommand{\airlinefigtwo}[1]{}
\newcommand{\olduptext}[1]{\hspace{-1cm} \raisebox{ 0.8cm}{ {#1}} \hspace{-0.75cm} }
\newcommand{\uptext}[1]{ \raisebox{1cm}{ #1} }

\frame[plain]{
\frametitle{Example}



\begin{tabular}{c}
%\airlinefig{01-airline-months_all}&\hspace{0.6cm}\olduptext{$=$} \hspace{-0.1cm}
%\airlinefig{01-airline-months_1} & \olduptext{$+$}
%\airlinefig{01-airline-months_2} & \olduptext{$+$}
%\airlinefig{/01-airline-months_3} \\
%\hspace{-3.5mm}
%\fbox{

%\hspace{-3.5mm}
\begin{tabular}{cc}
%\\[-0.7em]
%\fcolorbox{blue}{white}{
\includegraphics[trim=7.8cm 14.5cm 0cm 2cm, clip, width=0.5\columnwidth]{figures/airline-pages/pg_0002-crop} 
%}
 & \uptext{$=$} \\
 entire signal & 
\end{tabular}

\\
\begin{tabular}{p{4cm}cp{4cm}cp{4cm}}
\\%[1cm]
\includegraphics[trim=0.4cm 6cm 8.4cm 2.75cm, clip, width=0.3\columnwidth]{figures/airline-pages/pg_0003-crop} & 
\hspace{-0.5cm} \uptext{$+$} \hspace{-0.9cm} &  
\includegraphics[trim=0.4cm 6cm 8.4cm 2.88cm, clip, width=0.3\columnwidth]{figures/airline-pages/pg_0004-crop} & 
\hspace{-0.5cm} \uptext{$+$} \hspace{-0.9cm} &  
\includegraphics[trim=0.4cm 6cm 8.4cm 2.75cm, clip, width=0.3\columnwidth]{figures/airline-pages/pg_0005-crop} \\
{\small A very smooth monotonically increasing function }
& & 
{\small An approximately periodic function with a period of 1.0 years and with
approximately linearly increasing amplitude}
& & 
{\small An exactly periodic function with a period of 4.3 years but with linearly
increasing amplitude }
\end{tabular}
\end{tabular}
}


\frame[plain]{
\frametitle{Kernel Choice is Important}
\begin{itemize} 
	\item Kernel determines almost all the properties of the prior.
	\item Many different kinds, with very different properties:
\input{tables/simple_kernels_table_v3}
\end{itemize}
}


\frame[plain]{
\frametitle{Kernels can be composed}
\begin{itemize} 
	\item Two main operations: adding, multiplying
	\input{tables/comp1}
\end{itemize}
}

\frame[plain]{
\frametitle{Kernels can be composed}
\begin{itemize} 
	\item Can be composed across multiple dimensions
	\input{tables/comp2}
\end{itemize}
}



\frame[plain]{
\frametitle{Special Cases}
\begin{center}
  \begin{tabular}{l|l}
  Bayesian linear regression & $\Lin$ \\
  %Bayesian quadratric regression & $\Lin \times \Lin$ \\
  Bayesian polynomial regression & $\Lin \times \Lin \times \ldots$\\
  Generalized Fourier decomposition & $\Per + \Per + \ldots$ \\
  Generalized additive models & $\sum_{d=1}^D \SE_d$ \\
  Automatic relevance determination & $\prod_{d=1}^D \SE_d$ \\
  Linear trend with deviations & $\Lin + \SE$ \\
  Linearly growing amplitude & $\Lin \times \SE$
  \end{tabular}
\end{center}
}


\frame[plain]{
\frametitle{Appropriate kernels are necessary for extrapolation}
\begin{itemize} 
	\item SE kernel $\rightarrow$ basic smoothing.
	\item Richer kernels means richer structure can be captured.
\end{itemize}
\begin{center}
  \input{figures/fig_synth_extrap.tex}
\end{center}
}


\frame[plain]{
\frametitle{Kernels are hard to choose}
\begin{itemize}
	\item Given the diversity of priors available, how to choose one?
	\item Standard GP software packages include many base kernels and means to combine them, but \emph{no default kernel}
	\item Software can't choose model for you, you're the expert (?)
\end{itemize}
}

\frame[plain]{
\frametitle{Kernels are hard to construct}
\begin{itemize}
	\item Carl devotes 4 pages of his book to constructing a custom kernel for CO2 data
	\item requires specialized knowledge, trial and error, and a dataset small and low-dimensional enough that a human can interpret it.
	\item In practice, most users can't or won't make custom kernel, and SE kernel became \emph{de facto} standard kernel through inertia.
\end{itemize}
}


\frame[plain]{
\frametitle{Recap}
\begin{itemize}
	\item GP Regression is a powerful tool
	\item Kernel choice allows for rich structure to be captured - different kernels express very different model classes
	\item Composition generates a rich space of models
	\item Hard \& slow to search by hand
	\item Can kernel specification be automated?
\end{itemize}
}



\frame[plain]{
\frametitle{Compositional Structure Search}
\begin{itemize}
	\item Define grammar over kernels:
	\begin{itemize}
		\item $ K \rightarrow K + K$ 
		\item $ K \rightarrow K \times K$ 
		\item $ K \rightarrow \{ \SE, \RQ, \Lin, \Per \}$
	\end{itemize}
	\item Search the space of kernels greedily by applying production rules, checking model fit (approximate marginal likelihood).
\end{itemize}
}

\frame[plain]{
\frametitle{Compositional Structure Search}
\begin{center}
\input{figures/fig_search_tree}
\end{center}
}





\frame[plain]{
\frametitle{Example Search: Mauna Lua CO$_2$}
\begin{center}
  \input{figures/fig_mauna_growth.tex}
\end{center}
}




\frame[plain]{
\frametitle{Example Decomposition: Mauna Loa CO$_2$}
\begin{center}
  \input{figures/fig_mauna2.tex}
\end{center}
}

\frame[plain]{
\frametitle{Compound kernels are interpretable}
Suppose functions ${f_1, f_2}$ are draw from independent \gp{} priors, ${f_1 \sim \GP(\mu_1, k_1)}$, ${f_2 \sim \GP(\mu_2, k_2)}$.
Then it follows that $${f := f_1 + f_2 \sim \GP(\mu_1 + \mu_2, k_1 + k_2)}$$
Sum of kernels is equivalent to sum of functions.
Distributivity means we can write compound kernels as sums of products of base kernels:
$${\SE \times (\RQ + \Lin) = \SE \times \RQ + \SE \times \Lin}.$$
}

\frame[plain]{
\frametitle{Example Decomposition: Airline }
\begin{center}
  \input{figures/fig_airline.tex}
\end{center}
}

\frame[plain]{
\frametitle{Example: Sunspots}
\begin{center}
	\includegraphics[width=0.8\textwidth]{figures/solar}
\end{center}
}

\frame[plain]{
\frametitle{Changepoint Kernel}
Can express change in covariance:
\begin{center}
\only<1>{
	\includegraphics[width=0.5\textwidth]{figures/periodic_change_to_se_cov_1}
	\includegraphics[width=0.5\textwidth]{figures/periodic_change_to_se_draw_1}
	\\
	Periodic changing to SE
	}
\only<2>{	
	\includegraphics[width=0.5\textwidth]{figures/lin_change_to_se_cov_2}
	\includegraphics[width=0.5\textwidth]{figures/lin_change_to_se_draw_2}
	\\
	SE changing to linear
	}
\end{center}
}




\frame[plain]{
\frametitle{Summary}
\begin{itemize}
	\item Choosing form of kernel is currently done by hand.
	\item Compositions of kernels lead to more interesting priors on functions than typically considered.
	\item A simple grammar specifies all such compositions, and can be searched over automatically.
	\item Composite kernels lead to interpretable decompositions.
\end{itemize}
}



\frame[plain]{
\frametitle{Conclusions}

	\begin{itemize}
		\item Model-building is currently done mostly by hand.
		\item Grammars over composite structures are a simple way to specify open-ended model classes.
		\item Composite structures often imply interpretable decompositions of the data.
		\item Searching over these model classes is a step towards automating statistical analysis.
	\end{itemize}
	
	\pause
	\centering
	{
		\hfill
		Thanks!
				\hfill
	}
}




\end{document}