\documentclass{article} % For LaTeX2e
\usepackage{format/nips13submit_e}
\nipsfinalcopy % Uncomment for camera-ready version
\usepackage{times}
\usepackage{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{url}
\usepackage{color}
\usepackage{preamble}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
    
\usepackage{graphicx, amsmath, amsfonts, bm, lipsum, capt-of}

\usepackage{natbib, xcolor, wrapfig, booktabs, multirow, caption}

\usepackage{float}

\def\ie{i.e.\ }
\def\eg{e.g.\ }

\title{Automatic Summarization of Composite Nonparametric Time-Series Models}

\author{
James Robert Lloyd\\
University of Cambridge\\
Department of Engineering\\
\texttt{jrl44@cam.ac.uk}
\And
David Duvenaud\\
University of Cambridge \\
Department of Engineering \\
\texttt{dkd23@cam.ac.uk} \\
\And
Roger Grosse\\
M.I.T.\\
Brain and Cognitive Sciences \\
\texttt{rgrosse@mit.edu}
\And
Joshua B. Tenenbaum\\
M.I.T.\\
Brain and Cognitive Sciences \\
\texttt{jbt@mit.edu}
\And
Zoubin Ghahramani\\
University of Cambridge \\
Department of Engineering \\
\texttt{zoubin@eng.cam.ac.uk}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{0.9in}
\input{include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  % Note, NA's pass through!

\begin{document}

\allowdisplaybreaks

\maketitle

\begin{abstract}
To complement automatic model-building and search methods, we demonstrate an automatic model-summarization procedure.  Given a composite model, our method automatically constructs a report which visualizes and explains in words the meaning and relevance of each component of the model.  These reports enable human model-checking, understanding of complex modeling assumptions, and understanding of complex structure in data.
\end{abstract}

\section{Introduction}

%Statistical modeling is often performed in an iterative manner:  First, a simple model is proposed.  Second, the practitioner checks the model fit, often by looking for structure that wasn't captured by the model.


Parametric models such as linear regression usually have simple interpretations, and well-established methods for model-checking.  In constrast, complicated non-parametric models may be reasonably viewed with suspicion, since their assumptions may be difficult to check, and their predictive implications difficult to explain.

Advances in nonparametric and compositional modeling have allowed fully automatic model search and construction of complex, structured, nonprametric regression models \cite{DuvLloGroetal13}.  Such automatically-constructed models can be used for extrapolation without further human intevention, but if model-checking is desired, or human understanding is the goal, then simple extrapolations may be insufficient.  

This paper describes a procedure for automatically generating a human-readable report, examining and explaining the properties of a given nonparametric model on a given dataset.  The supplementary material to this paper is a report completely automatically generated by our method.  This report summarizes and describes a dataset after it has been modeled by a Gaussian process with a complicated kernel.  The various components of the model correspond to different features of the dataset, and the report discusses in detail the properties and significance of each component.

Section \ref{sec:gpss} gives an overview of the structure search method which produces the models that our procedure summarizes.  Section \ref{sec:method} describes our method for generating these reports, and section \ref{sec:example} examines in detail the report included in the supplementary material.


\section{Gaussian Process Structure Search}
\label{sec:gpss}

%\subsection{Gaussian process regression}
Gaussian process models\cite{rasmussen38gaussian} use a kernel to define the covariance between any two function values: ${\textrm{Cov}(\outputVar, \outputVar') = \kernel(\inputVar,\inputVar')}$.
%The kernel determines which sorts of structures the model places most of its probability mass upon, and in effect determines the generalization properties of the model.
The covariance function, or \emph{kernel}, specifies which structures are likely under the \gp{} prior, which in turn determines the generalization properties of the model.

Different kernels can express wide varieties of statistical structures, such as local smoothness, periodicity, additivity, and changepoints.  Compositions of kernels can express even richer structure, such as functions which are periodic only locally, or functions whose amplitude grows over time.  The Gaussian process structure search (GPSS) procedure simply searches over such compositions, in order to find complex structure in time-series or multidimensional functions \cite{DuvLloGroetal13}.

When given a dataset, GPSS produces a complex kernel which can usually be expressed as a sum of simpler kernels.  One property of Gaussian process models is that a sum of kernels is equivalent to a sum of functions.  This means that the models produced by GPSS can be interpreted as decomposing the function being modeled into a sum of parts.

%Each part of the kernel defines a covariance function, which in turn describes a prior probability distribution over functions.  Conditioned on the dataset, the covariance functions also define a 

\section{Automatic Report Generation}
\label{sec:method}

Our procedure takes as its starting point a dataset and a composite kernel, which together define a joint probability distribution over a sum of functions.  The function of our procedure is to illustrate the properties of this complex distribution to the user.

\subsection{Design goals}

\begin{itemize}
\item {\bf Intelligibility}
The procedure was designed to produce reports intelligible to non-experts.  

\item {\bf Illustrate Model Assumptions}
One of the main design criteria when designing our procedure was to make clear what assumptions the model is making, and what these assumptions imply in terms of extrapolation.  Even when simple Gaussian process models are used, it is often unclear what structure was captured by the model and what was not.

\item {\bf Illustrate Posterior Uncertainty}
One of the selling points of Bayesian modeling over point estimates is that they produce a rich posterior distribution over possible explanations of the data.  However, this posterior distribution is often quite a complex object, and is usually difficult to summarize.

\item {\bf Enable Model-checking}
One of the most important reasons to examine a model is to discover flawed assumptions, or structure not captured by the model.  Simply examining residuals, cross-validation or marginal likelihood scores is often not sufficient to notice when the model is failing to capture.
\end{itemize}

\subsection{Report structure}

Our procedure produces a report with three sections:

\begin{enumerate}
\item {\bf Executive summary} The first section of each report describes which sorts of structure were discovered in the model, and the relevant importance of the different components in explaining the data.
\item {\bf Discussion of additive components} The second section contains a detailed discussion of each component in turn.  Every component is plotted, and various properties of the statistical structure represent are described.  Because components with small variance are often not meaningful when plotted on their own, we also include plots of each component combined with all components of higher marginal variance.
\item {\bf Extrapolation} The third section shows extrapolations into the future, as well as posterior samples from each component of the model.  These samples help to characterize the uncertainty captured by the model, and the extent to which different components contribute to predicting the future behavior of the time series.
\end{enumerate}





\section{Example: Summarizing 400 Years of Solar Activity}
\label{sec:example}

To give an example of the capabilities of our procedure, here we show excerpts from the report automatically generated when run on annual solar irradiation data from 1610 to 2011.  This dataset has two obvious features:  First, a roughly 11-year cycle or solar activity, and second, a period from 1645 to 1715 with much smaller variance than the rest of the dataset.  This flat region corresponds to the Maunder minimum, a period in which sunspots were extremely rare \citep{lean1995reconstruction}.

\subsection{Executive Summary}

\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.98\columnwidth]{solarpages/02-solar-seperate-pages-2}}
\caption{
An example of an automatically-generated summary of a dataset.  The dataset is broken into a set of diverse types of structure, and each structure is explained in simple terms.}
\label{fig:exec}
\end{figure}

Figure \ref{fig:exec} shows the reported summary of the solar dataset.  The model discovers 9 seperate components which explain the data, and reports that the first 4 components explain 90\% of the variance in the data.  This might seem incongruous with the observation that there are two main features of the data, but if we examine the first four components, we see that the first component is explaining the mean of the dataset, the second explains the Maunder minimum, the third describes the long-term trend, and the fourth describes the 11-year period.


\subsubsection{Signal versus Noise}

One design challenge we encountered was seperating the recovered structure into signal and noise.  Originally, the model always included had a term corresponding to \iid{} additive Gaussian noise.  However, philosophically and in practice, the distunction between signal and noise in unclear for two reasons.  First, a component which varies arbitrarily quickly in time becomes indistinguishable from noise.  Second, the variance of the noise may change over time (called heteroskedasticity), and this sort of pattern may be considered part of the signal.

Because of the blurry distinction between signal and noise, we include a table which summarizes the relative contribution of each component in terms of predictive power.  To do this, we order the components in terms of how much each one improves predictive performance in a 10-fold cross-validation procedure.  The intuition for this metric is that noise-like components do not contribute much to the extrapolation performance of the model, but that signal-like components do.
%
%
\begin{figure}
\centering
\fbox{\includegraphics[width=0.98\columnwidth]{solarpages/02-solar-seperate-pages-3}}
\caption{A table summarizing the relative contribution of the 9 different components of the model in terms of predictive performance.}
\label{fig:table}
\end{figure}
%
Figure \ref{fig:table} show an example of this table on the solar dataset.

Because the user may be interested in local or noisy components, we report all components to the user.  An interactive version of our procedure could allow users to specify which components are of interest, and group the remaining components into a single noise component.


\subsection{Decomposition plots}

Because the individual components produced by our procedure are produced by an open-ended grammar, it is impossible to write special code to handle every type of component that GPSS might produce.  In addition, the posterior distribution on functions implied by the model depends on the data.  Hence the part of the procedure which summarizes the individual components must be flexible, and also must present enough information that users can see the meaning of each component themselves.  We summarize each component both in writing and in plots.

\paragraph{Automatic English Summarization}
For the written portion, a first paragraph reports the type of component found, and the meaning of all kernel parameters in that component.  This is possible even in an open-ended grammar of kernels because of the compositional nature of kernel structures.  Each type of kernel in an expression modifies the form of the posterior in a different way.  Table \ref{table:descriptions} gives some examples.

\begin{table}
\begin{tabular}{l|l}
Kernel & Description \\
\midrule
$\kSE$ & A smoothly varying function \\
$\kSE \times \kPer$ & A smoothly varying periodic function \\
$\kSE \times \kPer \times \kLin$ & A smoothly varying periodic function with growing amplitude \\
\end{tabular}
\caption{A demonstration of how it is possible to write an open-ended summarizer.  Different kernel structures modify the overall statistical structure of each component in independent ways.}
\label{table:descriptions}
\end{table}

A second paragraph explains the improvement in predictive performance gained by including this component in the model. This is the same informatino as included in the executive summary.

\paragraph{Automatic Plotting}
In the component plots, each component's posterior is described in two ways.  First, the posterior mean and variance of each component is plotted on its own.  Second, the posterior mean and variance of all components shown so far is plotted against the data.  By contrasting each of these plot with plots of earler components, we can see qualitatively how the current component is contributing to an overall explanation of the data.

\begin{figure}[h!]
\centering
\fbox{\includegraphics[width=0.98\columnwidth]{solarpages/02-solar-seperate-pages-5}}
\caption{Discovering the Maunder minimum.  The kernel found by GPSS contained a pair of changepoints bracketing the period of low solar activity.}
\label{fig:maunder}
\end{figure}

Figure \ref{fig:maunder} is a good example of a meaningful component discovered by GPSS, whose meaning would be unclear without an individual plot.  In the GPSS grammar, we allow any kernel expression to be multiplied by a changepoint kernel, which smoothly modulates the whole expression over time.  Two changepoints combined can encode a local change in covariance.

In the history of solar activity, the Maunder minimum is a good example of a local change in covariance.  Specifically, from about 1645 to 1715, solar activity decreased, and very few susnpots were observed \citep{lean1995reconstruction}.  In this example, GPSS captured this structure with a pair of changepoint kernels, and the plotted summary shows exactly which sort of structure was recovered by this component.

\begin{figure}[h!]
\centering
\fbox{\includegraphics[width=0.98\columnwidth]{solarpages/02-solar-seperate-pages-6}}
\caption{Characterizing the medium-term smoothness of solar activity levels.  By allowing other components to explain the periodicity, noise, and the Maunder minimum, we can isolate the part of the signal that's best explained by a slowly-varying trend.}
\label{fig:smooth}
\end{figure}

\paragraph{Isolating the smoothly-varying component} Examining the dataset by eye, overall solar activity seems to change slowly over decades.  However, this intuition seems difficult to formalize.  Linear or quadratic regression is clearly inappropriate, and methods based on local smoothing would need to control for the periodic component.  Luckily, the GPSS procedure does exactly this, allowing us to isolate the slowly-varying component of the data, without having to forecast either the Maunder minimum or the periodic variation.  Figure \ref{fig:smooth} shows the automatically-generated summary of this component.

\begin{figure}[h!]
\centering
\fbox{\includegraphics[width=0.98\columnwidth]{solarpages/02-solar-seperate-pages-7}}
\caption{Isolating the periodic component of the dataset.  By isolating this aspect of the statistical structure, we can easily observe additional features, such as the shape of the peaks and troughs, or the fact that the amplitude changes over time.}
\label{fig:periodic}
\end{figure}

Figure \ref{fig:periodic} shows the isolated periodic component of the signal.  Here we see one of the main benefits of isolating individual components: we can now see, by eye, extra structure that was not captured by the model.  Specifically, we can see that the amplitude of the periodic component varies over time, and by comparing with figure \ref{fig:smooth}, we can see that it varies roughly in proportion to the overall magnitude of the signal.  This pattern suggests that some sort of log-transform might be appropriate for this dataset, or that the model should be extended in some way to capture this structure.

\subsection{Extrapolation plots}

The predictive mean and variance of the signals shown in the summary plots are useful, but do not capture the joint correlation structure in the posterior.  Showing posterior samples are a simple and universal way to demonstrate the statistical structure captured by a model.

\begin{figure}[h!]
\centering
\fbox{\includegraphics[width=0.98\columnwidth]{solarpages/02-solar-seperate-pages-13}}
\caption{Sampling from the posterior.  These samples help show not just the predictive mean and variance, but also the predictive covariance structure.  Note, for example, that the predictive mean (left) doesn't exhibit periodicity, but the samples (right) do.}
\label{fig:extrap-full}
\end{figure}

For example, the left-hand plot in figure \ref{fig:extrap-full} shows the predictive mean and variance given the entire model. It is not clear from this plot whether or not the periodicity of the dataset is expected to continue into the future.  However, from the samples on the right-hand size, we can see that this is the case.  

\begin{figure}[h!]
\centering
\fbox{\includegraphics[width=0.98\columnwidth]{solarpages/02-solar-seperate-pages-16}}
\caption{Extrapolating a single component of the model.  Because our model class allows us to isolate individual components, we can show the extent to which the model expects different trends to continue.  We also observe that the posterior samples are quite different from the posterior mean, away from the data.}
\label{fig:extrap-smooth}
\end{figure}

\paragraph{Extrapolating individual components}
We can also examine the model's expectations about the future behavior of individual components through sampling.  For example, in figure \ref{fig:extrap-smooth}, we are shown samples of the predictive distribution of the smooth component of variation.  This plot indicates that the model considers a wide range of future average intensities to be plausible, but that it always expects those average intensities to vary smoothly.

\section{Related Work}

There exists a vast literature on both model visualization and model checking.


%\paragraph{Structure learning in Bayesian networks}
%Similar idea of discovering semantics via model search.
%Semantics are more vague though \ie a probability table is not an entirely concise summary

%\paragraph{Linear model}
%These discover highly interpretable semantics but are limited in expressivity

%\paragraph{Nonparametric additive models}
%Highly flexible but semantics are vague \ie can only talk about smooth functions

%\paragraph{Equation learning}
%Very flexible but semantics of equations do not map onto human understanding \eg saw tooth vs Fourier decomposition of a saw tooth - which is more human understandable?
%How would you explain a sensor error with Eureqa style equations.

%\paragraph{Deep learning}
%Again very flexible but the semantics are not usually human interpretable.
%How can we understand the output of complex representation learning algorithms without human intervention (\eg recognising that your deep net has become a cat classifier).

%\paragraph{Kernel search}
%Can use the precise semantics of linear models or the vague semantics of nonparametric additive models and other components along this spectrum.
%Flexible modelling with components that a human might use to describe what is going on.

\section{Discussion}

\begin{quotation}
``The availability of 'user-friendly' statistical software has caused authors to become increasingly careless about the logic of interpreting their results, and to rely uncritically on computer output, often using the 'default option' when something a little different (usually, but not always, a little more complicated) is correct, or at least more appropriate.''
% In trying to practice this art, the Bayesian has the advantage because his formal apparatus already developed gives him a clearer picture of what to expect, and therefore a sharper perception for recognizing the unexpected.

\defcitealias{dyke1997avoid}{G. Dyke, 1997}
%\hspace*{\fill}\citet{Jaynes85highlyinformative}
\hspace*{\fill}\citetalias{dyke1997avoid}
\end{quotation}

In this paper, we demonstrated a method for automatically summarizing a compositional Gaussian process model.  These summaries can enable human experts and non-experts to understand the implications of a model, check its plausibility, and notice structure not yet discovered by the model.


\bibliographystyle{unsrt}
\bibliography{gpss}

\end{document}
