\input{include/header_beamer}
\usepackage{etex}

\usepackage{tabularx}
\usepackage{include/picins}
\usepackage{include/preamble}
\usepackage{xcolor}
\usepackage{tikz}

\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Some look and feel definitions
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\columnsep}{0.03\textwidth}
\setlength{\columnseprule}{0.0018\textwidth}
\setlength{\parindent}{0.0cm}

\tikzstyle{mybox} = [draw=white, rectangle]
\tikzset{hide on/.code={\only<#1>{\color{white}}}}

\definecolor{camlightblue}{rgb}{0.601 , 0.8, 1}
\definecolor{camdarkblue}{rgb}{0, 0.203, 0.402}
\definecolor{camred}{rgb}{1, 0.203, 0}
\definecolor{camyellow}{rgb}{1, 0.8, 0}
\definecolor{lightblue}{rgb}{0, 0, 0.80}
\definecolor{white}{rgb}{1, 1, 1}
\definecolor{whiteblue}{rgb}{0.80, 0.80, 1}

\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\tabbox}[1]{#1}

\hypersetup{colorlinks=true,citecolor=blue}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% The talk
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Building an automatic statistician}

\author{
\includegraphics[height=0.2\textwidth]{figures/JamesLloyd4}
\qquad
\includegraphics[height=0.2\textwidth, trim=20mm 25mm 0mm 25mm, clip]{figures/david2}
\qquad
\includegraphics[height=0.2\textwidth]{figures/roger-photo}
\\
James Robert Lloyd, David Duvenaud, Roger Grosse,\\
\includegraphics[height=0.2\textwidth, trim=0mm 7mm 0mm 0mm, clip]{figures/josh2}
\qquad
\includegraphics[height=0.2\textwidth]{figures/zg2}\\
Joshua Tenenbaum, Zoubin Ghahramani
}

\begin{document}

\frame[plain] {
\titlepage
}

\begin{frame}{There is a growing need for data scientists}
  \begin{itemize}
    \item We live in an era of abundant data
    \vspace{\baselineskip}
    \item The McKinsey Global Institute claim
    \begin{itemize}
      \item \emph{``The United States alone faces a shortage of 140,000 to 190,000 people with analytical expertise and 1.5 million managers and analysts with the skills to understand and make decisions based on the analysis of big data.''}
    \end{itemize}
    \vspace{\baselineskip}
    \item Automating statistical modeling would mitigate this requirement and potentially have a large impact on fields relying on expert statisticians, machine learning researchers and data scientist
    \begin{itemize}
       \item \eg Computational biology
       \item \emph{Another canonical example} \ldots
     \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Ingredients of an automatic statistician}
  \begin{itemize}
    \item {\bf An open-ended language of models}
    \begin{itemize}
       \item Expressive enough to capture real-world phenomena\ldots
       \item \ldots and the techniques used by human statisticians
     \end{itemize}
    \vspace{\baselineskip}
    \item {\bf A search procedure}
    \begin{itemize}
       \item To efficiently explore the language of models
     \end{itemize}
    \vspace{\baselineskip}
    \item {\bf A principled method of evaluating models}
    \begin{itemize}
       \item Trading off complexity and fit to data
     \end{itemize}
    \vspace{\baselineskip}
    \item {\bf A procedure to automatically explain the models}
    \begin{itemize}
       \item Making the assumptions of the models explicit\ldots
       \item \ldots in a way that is intelligible to non-experts
     \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Defining a language of regression models}
  \begin{itemize}
    \item Regression consists of learning a function $f: \mathcal{X} \to \mathcal{Y}$ from inputs to outputs from example input / output pairs
    \vspace{\baselineskip}
    \item Language should include simple parametric forms\ldots
    \begin{itemize}
       \item \eg Linear functions, Polynomials, Exponential functions
     \end{itemize}
    \vspace{\baselineskip}
    \item \ldots as well as functions specified by high level properties
    \begin{itemize}
       \item \eg Smoothness, Periodicity
     \end{itemize}
    \vspace{\baselineskip}
    \item Inference should be tractable for all models in language
  \end{itemize}
\end{frame}

\begin{frame}{Bayesian modeling}
  Placeholder for TMS
  
  Intro to Bayes rule and use in statistics
\end{frame}

\begin{frame}{Bayesian linear regression}
  Placeholder for TMS
  
  Show example of Bayesian linear regression through origin
  
  Prior, Bayes rule, Posterior, Picture one data point at a time
\end{frame}

\begin{frame}{Bayesian linear regression}
  Placeholder for TMS
  
  Show how to rewrite this as a Gaussian distribution
  
  Talk about finite number of sampling points, and that this has a limit
\end{frame}

\begin{frame}{Gaussian process regression}
  Placeholder for TMS
  
  Change the covariance, shows samples of SE, and then show posterior
\end{frame}

\frame[plain]{
\frametitle{Conditional Posterior}
Placeholder for TMS

With SE kernel:
    \begin{figure}
        \includegraphics<1>[width=6cm]{figures/gp_demo/1d_posterior_and_0_data}
        \includegraphics<2>[width=6cm]{figures/gp_demo/1d_posterior_and_1_data}
        \includegraphics<3>[width=6cm]{figures/gp_demo/1d_posterior_and_2_data}
        \includegraphics<4>[width=6cm]{figures/gp_demo/1d_posterior_and_3_data}
        \includegraphics<5>[width=6cm]{figures/gp_demo/1d_posterior_and_4_data}
    \end{figure}
}

\begin{frame}{Gaussian process regression}
  Placeholder for TMS
  
  Therefore let's see how far we can go with kernel functions
\end{frame}

\begin{frame}{We can do this with Gaussian processes}
  \begin{itemize}
    \item \gp{}s are distributions over functions such that any
finite subset of function evaluations, $(f(x_1), f(x_2), \ldots
f(x_N))$, have a joint Gaussian distribution
    \vspace{\baselineskip}
    \item A \gp{} is completely specified by
    \begin{itemize}
      \item Mean function, $\mu(x)=\mathbb{E}(f(x))$
      \item Covariance / kernel function, $\kernel(x,x') = \Cov(f(x),f(x'))$
    \end{itemize}
  \end{itemize}
  \vspace{\baselineskip}
  \begin{tabular}{cccc}
    \includegraphics<1>[width=0.2\textwidth]{figures/gp_demo/1d_posterior_and_0_data} &
    \includegraphics<1>[width=0.2\textwidth]{figures/gp_demo/1d_posterior_and_1_data} &
    \includegraphics<1>[width=0.2\textwidth]{figures/gp_demo/1d_posterior_and_2_data} &
    \includegraphics<1>[width=0.2\textwidth]{figures/gp_demo/1d_posterior_and_3_data}
  \end{tabular}
\end{frame}

\begin{frame}{A language of Gaussian process kernels}
  \begin{itemize}
    \item Marginalizing over an unknown mean function can be equivalently
expressed as a zero-mean \gp{} with a new kernel
  \begin{itemize}
    \item Suppose, ${f(x) \,|\, a \,\sim\, \gp{}(a \times \mu(x), \kernel(x,x'))}$ where $a \,\sim\, \mathcal{N}(0,1)$
    \item Then equivalently, $f(x) \,\sim\, \gp{}(0, \mu(x)\mu(x') + \kernel(x,x'))$
  \end{itemize}
  \vspace{\baselineskip}
  \item We therefore define a language of \gp{} regression models by
specifying a {\bf language of kernels}
  \end{itemize}
\end{frame}

\begin{frame}{The atoms of our language}
  \input{figures/simple_kernels_table}
\end{frame}

\begin{frame}{The composition rules of our language}
\begin{itemize} 
	\item Two main operations: addition, multiplication
\end{itemize}
\input{figures/comp}
\end{frame}

\begin{frame}{Modeling changepoints}
  Assume $\textcolor{red}{f_1(x)} \sim GP(0,k_1)$ and $\textcolor{blue}{f_2(x)} \sim GP(0,k_2)$. Define:
\[
f(x) = (1-\sigma(x)) \textcolor{red}{f_1(x)} + \sigma(x) \textcolor{blue}{f_2(x)}
\]

where $\sigma$ is a sigmoid function between 0 and 1.

\vspace{\baselineskip}

Then $f \sim GP(0,k)$, where
\[
k(x,x') = (1-\sigma(x)) \, \textcolor{red}{k_1(x,x')}  \, (1-\sigma(x')) + \sigma(x) \,
\textcolor{blue}{k_2(x,x')} \, \sigma(x') 
\]

We can parametrise the location $\tau$ and abruptness $a$ of the changepoint by replacing
$\sigma(x)$  with $\sigma(a(x-\tau))$. \\

\vspace{\baselineskip}

{\it Intutively (in one-dimension), the function $f$ behaves like
$\textcolor{red}{f_1}$ before $\tau$ and like $\textcolor{blue}{f_2}$ after $\tau$. }
\end{frame}

\begin{frame}{An expressive language of models}
\begin{center}
\begin{tabular}{l|l}
Regression model & Kernel \\
\midrule
\gp{} smoothing & $\kSE + \kWN$ \\
Linear regression & $\kC + \kLin + \kWN$ \\
Multiple kernel learning & $\sum \kSE$ + \kWN\\
Trend, cyclical, irregular & $\sum \kSE + \sum \kPer$ + \kWN\\
Fourier decomposition & $\kC + \sum \cos$ + \kWN\\
Sparse spectrum \gp{}s & $\sum \cos$ + \kWN\\
Spectral mixture & $\sum \SE \times \cos$ + \kWN\\
Changepoints & \eg $\kCP(\kSE, \kSE) + \kWN$ \\
Heteroscedasticity & \eg $\kSE + \kLin \times \kWN$
\end{tabular}
\end{center}
Note: $\cos$ is a special case of our version of $\kPer$
\end{frame}

\begin{frame}{Exploring the language}
  \begin{itemize}
    \item The space spanned by the language is large and open-ended requiring a judicious search procedure
    \vspace{\baselineskip}
    \item We propose a greedy search for its simplicity and similarity to human model building
    \begin{itemize}
      \item Adding a new kernel corresponds to adding a new component to an additive model, similar to forward selection strategies
      \item Multiplying by a new kernel modifies an existing component of a model
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Example: Mauna Loa Keeling Curve}
\hspace{-1.2cm}
\only<1>{\includegraphics[width=0.4\textwidth]{figures/11-Feb-v4-03-mauna2003-s_max_level_0/03-mauna2003-s_all_small.pdf}}
\only<2>{\includegraphics[width=0.4\textwidth]{figures/11-Feb-v4-03-mauna2003-s_max_level_1/03-mauna2003-s_all_small.pdf}}
\only<3>{\includegraphics[width=0.4\textwidth]{figures/11-Feb-v4-03-mauna2003-s_max_level_2/03-mauna2003-s_all_small.pdf}}
\only<4>{\includegraphics[width=0.4\textwidth]{figures/11-Feb-v4-03-mauna2003-s_max_level_3/03-mauna2003-s_all_small.pdf}}

\vspace{-3.5cm}
\begin{minipage}[t][14cm][t]{1.14\linewidth}
\begin{flushleft}
\hspace{5.5cm}
\vspace{-8cm}
\makebox[\textwidth][c]{
\raisebox{10cm}{
\vspace{-8cm}
\begin{tikzpicture}
[sibling distance=0.18\columnwidth,-,thick, level distance=0.13\columnwidth]
%\footnotesize
\node[shape=rectangle,draw,thick] {Start}
%\pause
  child {node {$\SE$}}
%  fill=camlightblue!30
  child {node[shape=rectangle,draw,thick] {$\RQ$}
    [sibling distance=0.16\columnwidth]
%    {\visible<2->{ child {node {\ldots}}}}
    child [hide on=-1] {node {$\SE$ + \RQ}}
    child [hide on=-1] {node {\ldots}}
    child [hide on=-1] {node[shape=rectangle,draw,thick] {$\Per + \RQ$}
      [sibling distance=0.23\columnwidth]
      child [hide on=-2] {node {$\SE + \Per + \RQ$}}
      child [hide on=-2] {node {\ldots}}
      child [hide on=-2] {node[shape=rectangle,draw,thick] {$\SE \times (\Per + \RQ)$}
        [sibling distance=0.14\columnwidth]
        child [hide on=-3] {node {\ldots}}
        child [hide on=-3] {node {\ldots}}
        child [hide on=-3] {node {\ldots}}
      }
      child [hide on=-2] {node {\ldots}}
    }
    %child {node {$\RQ \times \SE$}}
    child [hide on=-1] {node {\ldots}}
    child [hide on=-1] {node {$\Per \times \RQ$}}
  }
  child {node {$\Lin$}}
  child {node {$\Per$}}
  ;
\end{tikzpicture}}
}\end{flushleft}
\end{minipage}
\only<4>{}
\end{frame}

\begin{frame}{Bayesian Occam's razor}
  Placeholder for TMS
  
  The classic model comparison slide for polynomials
\end{frame}

\begin{frame}{Model evaluation}
  \begin{itemize}
    \item After proposing a new model its kernel parameters are optimised by conjugate gradients
    \item We evaluate each optimized model, $M$, using the Bayesian Information Criterion (BIC):
\[
\textrm{BIC}(M) = -2 \log p(D\,|\, M) + p \log n
\]
where $p$ is the number of kernel parameters, $\log p(D|M)$ is the marginal likelihood of the data, $D$, and $n$ is the number of data points.
  \item BIC trades off model fit and complexity and implements what is known as ``Bayesian Occam's Razor'' 
  \end{itemize}
\end{frame}

\begin{frame}{Example: Airline passenger data}

\newcommand{\wmgd}{0.5\columnwidth}
\newcommand{\hmgd}{3.0cm}
\newcommand{\mdrd}{figures/01-airline}
\newcommand{\mbm}{\hspace{-0.3cm}}
\begin{tabular}{cc}
\mbm \includegraphics[width=\wmgd,height=\hmgd]{\mdrd/01-airline_raw_data} & \includegraphics[width=\wmgd,height=\hmgd]{\mdrd/01-airline_all}
\end{tabular}
\vspace{0.5\baselineskip}

{\scriptsize
Four additive components have been identified in the data
\begin{itemize}

  \item \input{\mdrd/01-airline_1_short_description.tex} 

  \item \input{\mdrd/01-airline_2_short_description.tex} 

  \item \input{\mdrd/01-airline_3_short_description.tex} 

  \item \input{\mdrd/01-airline_4_short_description.tex} 

\end{itemize}
}
\end{frame}

\begin{frame}{How can we describe a model}
  \begin{center}
  Suppose the search finds the following kernel
  \begin{align*}
    \kSE \times (\kWN \times \kLin + \kCP(\kC, \kPer))
  \end{align*}
  Give some motivation!
  \end{center}
\end{frame}

\begin{frame}{How can we describe a model}
  \begin{center}
  Suppose the search finds the following kernel
  \begin{align*}
    \kSE \times (\kWN \times \kLin + \kCP(\kC, \kPer))
  \end{align*}
  \end{center}
\end{frame}

\begin{frame}{How can we describe a model}
  \begin{center}
  The changepoint can be converted into a sum of products
  \begin{align*}
    \kSE \times (\kWN \times \kLin + \kC \times \boldsymbol{\sigma} + \kPer \times \boldsymbol{\bar\sigma})
  \end{align*}
  \end{center}
\end{frame}

\begin{frame}{How can we describe a model}
  \begin{center}
  Multiplication can be distributed over addition
  \begin{align*}
    \kSE \times \kWN \times \kLin + \kSE \times \kC \times \boldsymbol{\sigma} + \kSE \times \kPer \times \boldsymbol{\bar\sigma}
  \end{align*}
  \end{center}
\end{frame}

\begin{frame}{How can we describe a model}
  \begin{center}
  Simplification rules are applied
  \begin{align*}
    \kWN \times \kLin + \kSE \times \boldsymbol{\sigma} + \kSE \times \kPer \times \boldsymbol{\bar\sigma}
  \end{align*}
  \end{center}
\end{frame}

\begin{frame}{How can we describe a model}
  Sums of kernels are sums of products - maths and picture
\end{frame}

\begin{frame}{How can we describe a model}
  Show the build up of an $\kSE \times \kPer \times \kLin \times \boldsymbol\sigma$ kernel starting with the noun and adding modifiers
  
  Then show the underset kernel description
\end{frame}

\begin{frame}{Example: Airline}
\end{frame}

\begin{frame}{Example: Solar}
\end{frame}

\begin{frame}{Example: Call centre}
\end{frame}

\begin{frame}{Example: Sulphuric}
\end{frame}

\begin{frame}{Good predictive performance as well}
  Interpretability / accuracy trade-off rears its annoying head - but both are pretty good :)
\end{frame}

\begin{frame}{Challenges}
  
\end{frame}

\begin{frame}{Current and future directions}
  
\end{frame}

\begin{frame}{Summary}
  
\end{frame}

\end{document}

\begin{frame}{Title}
  \begin{itemize}
    \item Content
    \vspace{\baselineskip}
    \item Content
    \vspace{\baselineskip}
    \item Content
    \begin{itemize}
       \item Content
       \item Content
     \end{itemize}
  \end{itemize}
\end{frame}